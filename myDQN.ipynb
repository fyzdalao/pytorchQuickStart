{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### import和device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a080609ccab362db"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-16T07:04:08.933761900Z",
     "start_time": "2023-12-16T07:04:07.577978300Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T07:04:17.881320200Z",
     "start_time": "2023-12-16T07:04:17.875027300Z"
    }
   },
   "id": "cfeac2b19a9984d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reply Memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a583c7bdf93a7271"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T08:54:36.412706200Z",
     "start_time": "2023-12-16T08:54:36.386459600Z"
    }
   },
   "id": "dc673bcfa35e9c64"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_states, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T08:54:37.402733Z",
     "start_time": "2023-12-16T08:54:37.372758800Z"
    }
   },
   "id": "17c1a08329ff3970"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Transition(state=(1, 5), action=(2, 6), next_state=(3, 7), reward=(4, 8))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trs = [Transition(1, 2, 3, 4), Transition(5, 6, 7, 8)]\n",
    "trs = Transition(*zip(*trs))\n",
    "trs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T08:54:37.846244900Z",
     "start_time": "2023-12-16T08:54:37.838616300Z"
    }
   },
   "id": "9eb0534719e097af"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T08:54:38.353416900Z",
     "start_time": "2023-12-16T08:54:38.350908200Z"
    }
   },
   "id": "9403d544668217ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义Agent\n",
    "+ update_q_function()\n",
    "+ memorize()\n",
    "+ choose_function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83c4b9500f2949d3"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_states, n_actions, eta=0.5, gamma=0.99, capacity=10000, batch_size=32,\n",
    "                 eps_start=0.9, eps_end=0.05, eps_decay=1000):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity)\n",
    "        \n",
    "        self.policy_net = DQN(n_states, n_actions).to(device)\n",
    "        self.target_net = DQN(n_states, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return \n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # batch is Transition(state=(*,*,*,*), action=(*,*,*,*), next_state=(*,*,*,*), reward=(*,*,*,*)) \n",
    "        # there are 4 stars when batch_size=4\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "                                      device=device, dtype=torch.bool)\n",
    "        non_final_next_state_batch = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        \n",
    "        #pred: Q(state_t, action_t)\n",
    "        #truth: reward + \\gamma * \\max_a Q(state_{t+1},a)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(dim=1, index=action_batch)\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_state_batch).max(dim=1).values\n",
    "        expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "        \n",
    "        # x.unsqueeze(i)可以在x的第i维处增加一个维度\n",
    "        # expected_state_action_values的shapehe原本是(batch_size, )和reward_batch相同\n",
    "        # 改为(batch_size, 1)，和state_action_values相同\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def memorize(self, state, action, next_state, reward):\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "        math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if random.random() < eps_threshold:\n",
    "            # explore\n",
    "            return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # .max(1)就是取每一行最大的元素。model的输出只有一行，两列，所以也就是取两个数里更大的那个。\n",
    "                # .indices也可以写成下标[1]。要取的是最优的动作而不是这个动作的价值，所以要的是下标而不是具体的值。\n",
    "                # view函数是用来reshape的。view(1,1)就是搞成1*1的tensor\n",
    "                return self.policy_net(state).max(1).indices.view(1,1)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T11:35:43.092459300Z",
     "start_time": "2023-12-16T11:35:43.067516200Z"
    }
   },
   "id": "a32214608250b954"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agent与环境交互（训练过程）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b471afc515811210"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02023418  0.01963305  0.04826971  0.02622762]\n",
      "tensor([-0.0202,  0.0196,  0.0483,  0.0262], device='cuda:0')\n",
      "tensor([[-0.0202,  0.0196,  0.0483,  0.0262]], device='cuda:0')\n",
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# this cell is for test\n",
    "state, info = env.reset()\n",
    "print(state)\n",
    "state1 = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "print(state1)\n",
    "state2 = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "print(state2)\n",
    "print(state1.shape)\n",
    "print(state2.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T11:35:44.452667Z",
     "start_time": "2023-12-16T11:35:44.428323900Z"
    }
   },
   "id": "33ae988160812f61"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02329497 -0.23557281 -0.03836909  0.32001054]\n",
      "tensor([[ 0.0233, -0.2356, -0.0384,  0.3200]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# this cell is for test\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "agent = Agent(n_states, n_actions)\n",
    "\n",
    "state, info = env.reset()\n",
    "#unsqueeze之前state.shape是[4]，之后是[1,4]\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "action = agent.choose_action(state)\n",
    "observation, reward, is_done, _, _ = env.step(action.item())\n",
    "print(observation)\n",
    "next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "print(next_state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T11:35:45.022045Z",
     "start_time": "2023-12-16T11:35:45.001244800Z"
    }
   },
   "id": "bcfc46c6d8e2220d"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 9\n",
      "steps: 19\n",
      "steps: 14\n",
      "steps: 20\n",
      "steps: 29\n",
      "steps: 14\n",
      "steps: 18\n",
      "steps: 10\n",
      "steps: 34\n",
      "steps: 16\n",
      "steps: 12\n",
      "steps: 18\n",
      "steps: 11\n",
      "steps: 15\n",
      "steps: 22\n",
      "steps: 31\n",
      "steps: 57\n",
      "steps: 19\n",
      "steps: 13\n",
      "steps: 18\n",
      "steps: 16\n",
      "steps: 14\n",
      "steps: 13\n",
      "steps: 10\n",
      "steps: 27\n",
      "steps: 10\n",
      "steps: 12\n",
      "steps: 9\n",
      "steps: 25\n",
      "steps: 11\n",
      "steps: 18\n",
      "steps: 11\n",
      "steps: 16\n",
      "steps: 13\n",
      "steps: 15\n",
      "steps: 13\n",
      "steps: 10\n",
      "steps: 11\n",
      "steps: 11\n",
      "steps: 15\n",
      "steps: 13\n",
      "steps: 9\n",
      "steps: 16\n",
      "steps: 12\n",
      "steps: 9\n",
      "steps: 12\n",
      "steps: 13\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 14\n",
      "steps: 9\n",
      "steps: 20\n",
      "steps: 15\n",
      "steps: 9\n",
      "steps: 11\n",
      "steps: 12\n",
      "steps: 19\n",
      "steps: 10\n",
      "steps: 11\n",
      "steps: 9\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 20\n",
      "steps: 9\n",
      "steps: 11\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 14\n",
      "steps: 22\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 14\n",
      "steps: 11\n",
      "steps: 12\n",
      "steps: 11\n",
      "steps: 12\n",
      "steps: 15\n",
      "steps: 11\n",
      "steps: 9\n",
      "steps: 9\n",
      "steps: 13\n",
      "steps: 11\n",
      "steps: 13\n",
      "steps: 11\n",
      "steps: 15\n",
      "steps: 15\n",
      "steps: 9\n",
      "steps: 16\n",
      "steps: 14\n",
      "steps: 15\n",
      "steps: 21\n",
      "steps: 13\n",
      "steps: 10\n",
      "steps: 12\n",
      "steps: 24\n",
      "steps: 42\n",
      "steps: 38\n",
      "steps: 10\n",
      "steps: 28\n",
      "steps: 19\n",
      "steps: 11\n",
      "steps: 10\n",
      "steps: 31\n",
      "steps: 35\n",
      "steps: 14\n",
      "steps: 37\n",
      "steps: 27\n",
      "steps: 12\n",
      "steps: 11\n",
      "steps: 9\n",
      "steps: 27\n",
      "steps: 29\n",
      "steps: 23\n",
      "steps: 15\n",
      "steps: 25\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 33\n",
      "steps: 28\n",
      "steps: 11\n",
      "steps: 13\n",
      "steps: 9\n",
      "steps: 14\n",
      "steps: 12\n",
      "steps: 11\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 13\n",
      "steps: 14\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 9\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 11\n",
      "steps: 10\n",
      "steps: 11\n",
      "steps: 19\n",
      "steps: 14\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 11\n",
      "steps: 9\n",
      "steps: 10\n",
      "steps: 18\n",
      "steps: 10\n",
      "steps: 29\n",
      "steps: 17\n",
      "steps: 10\n",
      "steps: 11\n",
      "steps: 9\n",
      "steps: 11\n",
      "steps: 9\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 8\n",
      "steps: 10\n",
      "steps: 8\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 10\n",
      "steps: 11\n",
      "steps: 38\n",
      "steps: 9\n",
      "steps: 13\n",
      "steps: 9\n",
      "steps: 12\n",
      "steps: 12\n",
      "steps: 11\n",
      "steps: 12\n",
      "steps: 28\n",
      "steps: 11\n",
      "steps: 8\n",
      "steps: 10\n",
      "steps: 20\n",
      "steps: 14\n",
      "steps: 18\n",
      "steps: 10\n",
      "steps: 10\n",
      "steps: 9\n",
      "steps: 10\n",
      "steps: 16\n",
      "steps: 12\n",
      "steps: 12\n",
      "steps: 17\n",
      "steps: 37\n",
      "steps: 149\n",
      "steps: 140\n",
      "steps: 145\n",
      "steps: 94\n",
      "steps: 168\n",
      "steps: 119\n",
      "steps: 132\n",
      "steps: 172\n",
      "steps: 188\n",
      "steps: 168\n",
      "steps: 265\n",
      "steps: 326\n",
      "steps: 324\n",
      "steps: 205\n",
      "steps: 171\n",
      "steps: 290\n",
      "steps: 180\n",
      "steps: 175\n",
      "steps: 160\n",
      "steps: 167\n",
      "steps: 174\n",
      "steps: 208\n",
      "steps: 167\n",
      "steps: 147\n",
      "steps: 208\n",
      "steps: 155\n",
      "steps: 142\n",
      "steps: 171\n",
      "steps: 134\n",
      "steps: 171\n",
      "steps: 160\n",
      "steps: 184\n",
      "steps: 156\n",
      "steps: 193\n",
      "steps: 172\n",
      "steps: 160\n",
      "steps: 140\n",
      "steps: 179\n",
      "steps: 156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m policy_dict:\n\u001B[0;32m     36\u001B[0m     target_dict[key] \u001B[38;5;241m=\u001B[39m policy_dict[key]\u001B[38;5;241m*\u001B[39mtau \u001B[38;5;241m+\u001B[39m target_dict[key]\u001B[38;5;241m*\u001B[39m(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mtau)\n\u001B[1;32m---> 37\u001B[0m agent\u001B[38;5;241m.\u001B[39mtarget_net\u001B[38;5;241m.\u001B[39mload_state_dict(target_dict)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_done:\n\u001B[0;32m     40\u001B[0m     episode_durations\u001B[38;5;241m.\u001B[39mappend(t\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mW:\\Dev\\Anaconda\\envs\\pytorch_py3115\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2138\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[0;32m   2131\u001B[0m         out \u001B[38;5;241m=\u001B[39m hook(module, incompatible_keys)\n\u001B[0;32m   2132\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, (\n\u001B[0;32m   2133\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2134\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2135\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit should be done inplace.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2136\u001B[0m         )\n\u001B[1;32m-> 2138\u001B[0m load(\u001B[38;5;28mself\u001B[39m, state_dict)\n\u001B[0;32m   2139\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m load\n\u001B[0;32m   2141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strict:\n",
      "File \u001B[1;32mW:\\Dev\\Anaconda\\envs\\pytorch_py3115\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2126\u001B[0m, in \u001B[0;36mModule.load_state_dict.<locals>.load\u001B[1;34m(module, local_state_dict, prefix)\u001B[0m\n\u001B[0;32m   2124\u001B[0m         child_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2125\u001B[0m         child_state_dict \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m local_state_dict\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k\u001B[38;5;241m.\u001B[39mstartswith(child_prefix)}\n\u001B[1;32m-> 2126\u001B[0m         load(child, child_state_dict, child_prefix)\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001B[39;00m\n\u001B[0;32m   2129\u001B[0m incompatible_keys \u001B[38;5;241m=\u001B[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001B[1;32mW:\\Dev\\Anaconda\\envs\\pytorch_py3115\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2120\u001B[0m, in \u001B[0;36mModule.load_state_dict.<locals>.load\u001B[1;34m(module, local_state_dict, prefix)\u001B[0m\n\u001B[0;32m   2118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m assign:\n\u001B[0;32m   2119\u001B[0m     local_metadata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124massign_to_params_buffers\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m assign\n\u001B[1;32m-> 2120\u001B[0m module\u001B[38;5;241m.\u001B[39m_load_from_state_dict(\n\u001B[0;32m   2121\u001B[0m     local_state_dict, prefix, local_metadata, \u001B[38;5;28;01mTrue\u001B[39;00m, missing_keys, unexpected_keys, error_msgs)\n\u001B[0;32m   2122\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   2123\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mW:\\Dev\\Anaconda\\envs\\pytorch_py3115\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2040\u001B[0m, in \u001B[0;36mModule._load_from_state_dict\u001B[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001B[0m\n\u001B[0;32m   2038\u001B[0m                 \u001B[38;5;28msetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, input_param)\n\u001B[0;32m   2039\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2040\u001B[0m             param\u001B[38;5;241m.\u001B[39mcopy_(input_param)\n\u001B[0;32m   2041\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[0;32m   2042\u001B[0m     error_msgs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhile copying the parameter named \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2043\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose dimensions in the model are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2044\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose dimensions in the checkpoint are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_param\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2045\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124man exception occurred : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mex\u001B[38;5;241m.\u001B[39margs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2046\u001B[0m                       )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "max_episodes = 500\n",
    "tau = 0.005\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "agent = Agent(n_states, n_actions)\n",
    "\n",
    "finish_flag = False\n",
    "for i_episode in range(max_episodes):\n",
    "    state, info = env.reset()\n",
    "    #unsqueeze之前state.shape是[4]，之后是[1,4]\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for t in count():\n",
    "        # the choose_action() might be wrong\n",
    "        action = agent.choose_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        is_done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        agent.memorize(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        agent.optimize_model()\n",
    "        \n",
    "        target_dict = agent.target_net.state_dict()\n",
    "        policy_dict = agent.policy_net.state_dict()\n",
    "        for key in policy_dict:\n",
    "            target_dict[key] = policy_dict[key]*tau + target_dict[key]*(1-tau)\n",
    "        agent.target_net.load_state_dict(target_dict)\n",
    "        \n",
    "        if is_done:\n",
    "            episode_durations.append(t+1)\n",
    "            print(f'steps: {t+1}')\n",
    "            break\n",
    "    \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T11:36:03.635667800Z",
     "start_time": "2023-12-16T11:35:46.076437100Z"
    }
   },
   "id": "4a88416914568aa3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24d1bbd4f2367f5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
